import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from PIL import Image

# --- PAGE CONFIGURATION ---
st.set_page_config(layout="wide", page_title="Thumbnail Vision AI")

st.title("ðŸ‘ï¸ Thumbnail Vision: AI vs Human")
st.markdown("""
**Compare how an algorithm sees your thumbnail vs. how a real human looks at it.**
""")

# --- INITIALIZE SESSION STATE ---
# We need this to share the image between the two tabs
if "uploaded_image_rgb" not in st.session_state:
    st.session_state.uploaded_image_rgb = None

# --- TABS SETUP ---
tab_ai, tab_human = st.tabs(["ðŸ¤– AI Attention Prediction", "ðŸ‘€ Human Real-Time Tracking"])

# ==========================================
# TAB 1: AI PREDICTION (Static Analysis)
# ==========================================
with tab_ai:
    st.header("Step 1: AI Analysis")
    st.write("Upload your thumbnail to generate an artificial saliency map.")
    
    uploaded_file = st.file_uploader("Upload Thumbnail (JPG/PNG)", type=['jpg', 'png', 'jpeg'])

    if uploaded_file:
        # 1. Load Image
        image_pil = Image.open(uploaded_file).convert('RGB')
        image_np = np.array(image_pil)
        
        # Save to session state for Tab 2 use
        st.session_state.uploaded_image_rgb = image_np
        
        # 2. Process AI Saliency
        # Convert to BGR for OpenCV
        img_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
        
        # Saliency Algorithm (Spectral Residual)
        saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
        (success, saliency_map) = saliency.computeSaliency(img_bgr)
        
        if success:
            # Normalize and Colorize
            heatmap_ai = cv2.applyColorMap((saliency_map * 255).astype("uint8"), cv2.COLORMAP_JET)
            
            # Blend
            overlay_ai = cv2.addWeighted(img_bgr, 0.6, heatmap_ai, 0.4, 0)
            
            # 3. Display Results
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Original Image")
                st.image(image_pil, use_container_width=True)
            with col2:
                st.subheader("AI Prediction Heatmap")
                st.image(cv2.cvtColor(overlay_ai, cv2.COLOR_BGR2RGB), use_container_width=True)
                st.caption("Red areas indicate high probability of attracting attention.")

# ==========================================
# TAB 2: HUMAN TRACKING (Webcam Interactivity)
# ==========================================
with tab_human:
    st.header("Step 2: Human Eye Tracker")
    
    if st.session_state.uploaded_image_rgb is None:
        st.warning("âš ï¸ Please upload an image in the 'AI Attention Prediction' tab first.")
    else:
        st.info("""
        **Instructions:** 1. Click 'START' below. Allow camera access.
        2. The video will show your THUMBNAIL (not your face).
        3. Look at the left, center, or right of your screen. 
        4. See the heatmap build up in real-time based on your gaze.
        """)

        # Initialize MediaPipe
        mp_face_mesh = mp.solutions.face_mesh
        face_mesh = mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True, # Crucial for Iris detection
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # --- VIDEO PROCESSOR CLASS ---
        class HeatmapProcessor(VideoTransformerBase):
            def __init__(self):
                # Load the image from Session State
                self.target_rgb = st.session_state.uploaded_image_rgb
                self.h, self.w, _ = self.target_rgb.shape
                
                # Create an empty float matrix for heat accumulation
                self.heat_matrix = np.zeros((self.h, self.w), dtype=np.float32)
                
            def transform(self, frame):
                # 1. Get Webcam Frame
                img_cam = frame.to_ndarray(format="bgr24")
                
                # 2. Face/Eye Detection
                results = face_mesh.process(cv2.cvtColor(img_cam, cv2.COLOR_BGR2RGB))
                
                gaze_zone = "WAITING..."
                
                if results.multi_face_landmarks:
                    landmarks = results.multi_face_landmarks[0].landmark
                    
                    # --- GAZE LOGIC (Horizontal) ---
                    # Using Left Eye landmarks: 362 (Inner), 263 (Outer), 474 (Iris Center)
                    # Note: "Inner" is practically to the right on the screen, "Outer" to the left.
                    
                    x_inner = landmarks[362].x
                    x_outer = landmarks[263].x
                    x_iris = landmarks[474].x
                    
                    # Calculate distances
                    eye_width = abs(x_inner - x_outer)
                    iris_pos = abs(x_iris - x_outer)
                    
                    # Ratio: 0.0 (Looking Left) -> 1.0 (Looking Right)
                    if eye_width > 0:
                        ratio = iris_pos / eye_width
                    else:
                        ratio = 0.5
                    
                    # --- UPDATE HEATMAP MATRIX ---
                    # We add value to the matrix zone corresponding to the gaze
                    heat_increment = 3.0 # How fast the red color appears
                    
                    # Define 3 simple zones
                    if ratio < 0.44:
                        gaze_zone = "LOOKING LEFT"
                        self.heat_matrix[:, :int(self.w/3)] += heat_increment
                    elif ratio > 0.56:
                        gaze_zone = "LOOKING RIGHT"
                        self.heat_matrix[:, int(self.w*2/3):] += heat_increment
                    else:
                        gaze_zone = "LOOKING CENTER"
                        self.heat_matrix[:, int(self.w/3):int(self.w*2/3)] += heat_increment
                
                else:
                    gaze_zone = "NO FACE DETECTED"

                # 3. RENDER HEATMAP
                # Normalize matrix to 0-255
                max_val = np.max(self.heat_matrix)
                if max_val > 0:
                    # Scaling factor (prevents image from getting stuck at full red immediately)
                    scale = 255.0 / max(max_val, 50.0) 
                    norm_map = (self.heat_matrix * scale).astype(np.uint8)
                else:
                    norm_map = self.heat_matrix.astype(np.uint8)
                
                # Gaussian Blur for "cloudy" effect
                blurred_map = cv2.GaussianBlur(norm_map, (151, 151), 0)
                
                # Apply Color Map (Blue=Cold, Red=Hot)
                color_heatmap = cv2.applyColorMap(blurred_map, cv2.COLORMAP_JET)
                
                # 4. BLEND WITH THUMBNAIL
                # Convert thumbnail to BGR for OpenCV
                target_bgr = cv2.cvtColor(self.target_rgb, cv2.COLOR_RGB2BGR)
                
                # Blend: 60% Original Image, 40% Heatmap
                final_image = cv2.addWeighted(target_bgr, 0.6, color_heatmap, 0.4, 0)
                
                # Add Text Overlay
                cv2.putText(final_image, f"STATUS: {gaze_zone}", (30, 60), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                
                return final_image

        # --- RENDER WEBRTC ---
        webrtc_streamer(
            key="heatmap-stream",
            video_processor_factory=HeatmapProcessor,
            media_stream_constraints={"video": True, "audio": False},
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
        )
